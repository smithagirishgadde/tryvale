# Table of Contents

- [Table of Contents](#table-of-contents)
  - [Edge Video Analytics Microservice](#edge-video-analytics-microservice)
    - [Overview](#overview)
    - [Features](#features)
    - [Additional Resources](#additional-resources)
  - [Sample Workflow](#sample-workflow)
    - [Helm Chart](#helm-chart)
    - [How it works](#how-it-works)
    - [Running the workflow](#running-the-workflow)
    - [Modifying EVAM pipeline](#modifying-evam-pipeline)
## Edge Video Analytics Microservice

### Overview

Video analytics involves the conversion of video streams into valuable insights through the application of video processing, inference, and analytics operations. It finds applications in various business sectors including healthcare, retail, entertainment, and industrial domains. The algorithms utilized in video analytics are responsible for performing tasks such as object detection, classification, identification, counting, and tracking on the input video stream.

To enable the efficient development and deployment of optimized video analytics pipelines,  Edge Video Analytics Microservice (EVAM) is designed as an interoperable containerized microservice developed in Python programing language. It is built on top of GStreamer and Intel® Deep Learning Streamer (DL Streamer), which provide video ingestion and deep learning inferencing functionalities respectively. Developers can save development and deployment time by using the pre-built Docker* image and by simply configuring the video analytics pipelines in the well-known JSON format. 
 
EVAM in Edge Insights for Industrial (EII) mode  supports EII's Configuration Manager for pipeline execution and EII Message Bus for publishing of inference results, making it compatible with the Edge Insights for Industrial software stack. EVAM allows unrestriced customization and manipulation of video frames and metadata generated by the pipelines by letting developers add *User Defined Functions (UDFs)* at any stage in the pipelines. *UDFs* can be developed in either Python or C++. 

  <div align="center">
     <p align="center">
        <img src="./assets/edge-video-analytics-microservice-eii.png" width=65%>
        <figcaption align="center">EVAM Architecture</figcaption>
     </p>
  </div>

### Features
   - **Plugin to run User Defined Functions**</br>
      An User Defined Function (UDF) is a chunk of user code that can transform video frames and/or manipulate metadata. For example, a UDF can act as filter, preprocessor, classifier or a detector. These User Defined Functions can be developed in C++ or Python. A detailed guide for developing UDFs is available [here](./docs/howto_guide_for_writing_udfs.md). EVAM provides a GStreamer plugin - `udfloader` using which users can configure and load arbitrary UDFs. These UDFs are then become a pipeline stage and are called once for each video frame. Vist [udfloader docs](./docs/gst_udfloader.md) to learn more about configuring and using the `udfloader` element.
   - **Supports Publish over EII Message Bus (ZMQ)**<br>
      EVAM comes witha EII message bus publisher pre-configured for publishing the processed frames and metadata from the video analytics pipeline.
   - **Supports Subscribe over EII message bus (ZMQ)**<br>
     EVAM can be configured to subscribe frames/metadata from a EII message publisher. In this state EVAM uses the frames received from the EII message bus as
     input to the video analytics pipeline.
   - **Dynamic Configuration Change**</br>
      EVAM support dynamic configuration change using the Etcd UI provided by the EII Configmanager Agent. EVAM uses the Configmanager Agent APIs to detect any change in the config and restarts itself upon detecting a change. Visit [configuring evam](./docs/changing_configuration.md) to understand more about EVAM configuration.

### Additional Resources
 - GStreamer
   - [GStreamer Documentation](https://gstreamer.freedesktop.org/documentation/)
   - [GStreamer Plugins](https://gstreamer.freedesktop.org/documentation/plugins_doc.html?gi-language=c)
 - DLStreamer
   - [DLStreamer Documentation](https://dlstreamer.github.io/)
   - [DLStreamer Source](https://github.com/dlstreamer/dlstreamer)
 - Intel® OpenVINO™
   - [OpenVINO™ toolkit Reference](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html)
   - [OpenVINO™ Documentation](https://docs.openvino.ai/2023.0/home.html)
 - Intel® Geti™
   - [Geti™ Reference](https://geti.intel.com/)
## Sample Workflow
This sample workflow is designed to demonstrate running and customizing a Object Detection pipeline using the available Configuration Management Etcd UI. This sample also showcases the use of *UDFs* to run inference on a model trained and exported using [Intel® Geti™](https://geti.intel.com/).
 

>The outputs from the pipeline (frames and detection results) are published to the Multimodal Data Visualization Streaming microservice over a EII Message Bus publisher and can observed by visiting the Visualizer tab (Please open [link.md](./link.md) to get the link).

  <div align="center">
     <p align="center">
        <img src="./assets/edge-video-analytics-microservice-eii_workflow.png" width=65%>
        <figcaption>Workflow Architecture</figcaption>
     </p>
  </div>
  
### Helm Chart</br>
Helm chart for the sample workflow is available [here](./evam-eii-helm-chart)

### How it works</br>
The sample is deployed and configured using the helm chart available above. As the sample loads up, Edge Video Analytics Microservice and rest of the EII stack containers are brought up. EVAM container starts and executes the pipeline configured in the config file.

The outputs from the pipeline (frames and detection results) are published over the EII message bus, and are available for subscriber like the Visualizer microservice. The outputs can visualized by opening the link for Visualizer (open link.md to get the link).

The person detection sample workflow is realized using the pipeline given below. The key stages in the pipeline are -

 - Input Video Decoding
 - NV12 to BGR conversion
 - Person Detection model inference

The udfloader element here runs the person detection model using the inference scripts available in the Geti™ deployment folder.

**Person Detection Sample Pipeline Definition**

  ```
    multifilesrc loop=TRUE stop-index=0 location=/home/pipeline-server/resources/people-detection.avi name=source ! \
    h264parse ! decodebin ! videoconvert ! \
    video/x-raw,format=BGR ! \
    udfloader name=udfloader ! appsink name=destination
  ```

### Running the workflow

  <div align="center">
     <p align="center">
        <img src="./assets/edge-video-analytics-microservice-eii-etcdui.png" width=65%>
        <figcaption>Etcd UI</figcaption>
     </p>
  </div>

### Modifying EVAM pipeline
In ETCD-UI (open [link.md](./link.md) to get the link), in the Etcd UI open "/EdgeVideoAnalyticsMicroservice/config" to view and modify EVAM pipeline configs

  1. **Modify input video file** 
     Update line-13 with below line, and save the config. It will replace the existing video with another video of a classroom and will restart the pipeline.

    ```bash
    pipeline": "multifilesrc loop=TRUE stop-index=0 location=/home/pipeline-server/resources/classroom.avi name=source ! h264parse ! decodebin ! videoconvert ! video/x-raw,format=BGR ! udfloader name=udfloader ! appsink name=destination",
    ```

  2. **Updating UDF configuration**
     Update line-24 as below and save the config, It will enable blurring of detected objects.
    ```bash
      "blur_detections": true
    ```
